# MVP-007: Implement Database Infrastructure

## Ticket Overview

**Ticket ID:** MVP-007  
**Title:** Implement Database Infrastructure  
**Type:** Development  
**Description:** Set up the database infrastructure according to the data model defined in MVP-004. Create database connection pools, data access layers, and implement caching strategies.  
**Owner:** Anil  
**Reviewers:** Kushal, Rohith  
**Story Points:** 8  
**Priority:** High  
**Dependencies:** MVP-004, MVP-006  

## The Story Behind the Ticket

### Why This Ticket Matters

After defining our data model in MVP-004 and implementing authentication in MVP-006, MVP-007 focused on building the database infrastructure that would power our entire application. This ticket was about turning our database schema definitions into a robust, efficient, and maintainable database layer.

This ticket was essential because:

1. It established the database connection management for all services
2. It implemented the data access layer (DAL) that would be used by all services
3. It set up caching strategies to optimize performance
4. It provided a consistent approach to database operations across the application

A well-designed database infrastructure is crucial for application performance, scalability, and maintainability. MVP-007 was about getting this foundation right from the start, ensuring that all services could interact with the database in a consistent and efficient way.

### The Technical Implementation

#### 1. Database Connection Management

We implemented a robust database connection management system for PostgreSQL, MongoDB, and Redis:

```javascript
// Database configuration
const { Pool } = require('pg');
const mongoose = require('mongoose');
const Redis = require('ioredis');
const { logger } = require('../utils/logger');

// PostgreSQL Configuration
const pgConfig = {
  host: process.env.DB_HOST || 'localhost',
  port: parseInt(process.env.DB_PORT || '5432', 10),
  database: process.env.DB_NAME || 'dr_assistant',
  user: process.env.DB_USER || 'postgres',
  password: process.env.DB_PASSWORD || 'postgres',
  max: parseInt(process.env.DB_POOL_MAX || '20', 10),
  idleTimeoutMillis: parseInt(process.env.DB_POOL_IDLE_TIMEOUT || '30000', 10),
  connectionTimeoutMillis: parseInt(process.env.DB_POOL_CONNECTION_TIMEOUT || '5000', 10),
  ssl: process.env.DB_SSL === 'true' ? {
    rejectUnauthorized: process.env.DB_SSL_REJECT_UNAUTHORIZED !== 'false'
  } : false
};

// Create PostgreSQL pool
const pgPool = new Pool(pgConfig);

// PostgreSQL error handling
pgPool.on('error', (err) => {
  logger.error('Unexpected error on idle PostgreSQL client', { error: err.message, stack: err.stack });
});

// MongoDB connection
let mongoConnection = null;
const connectMongo = async () => {
  if (mongoConnection) return mongoConnection;
  
  try {
    mongoConnection = await mongoose.connect(mongoConfig.uri, mongoConfig.options);
    logger.info('MongoDB connected successfully');
    
    // MongoDB connection error handling
    mongoose.connection.on('error', (err) => {
      logger.error('MongoDB connection error', { error: err.message, stack: err.stack });
    });
    
    return mongoConnection;
  } catch (error) {
    logger.error('MongoDB connection failed', { error: error.message, stack: error.stack });
    throw error;
  }
};

// Create Redis client
const redisClient = new Redis(redisConfig);
```

This implementation included:
- Connection pooling for PostgreSQL
- Lazy connection for MongoDB
- Robust error handling
- Configuration through environment variables
- Graceful connection closing

#### 2. Repository Pattern Implementation

We implemented the repository pattern to provide a consistent interface for database operations:

```javascript
class BaseRepository {
  constructor(tableName) {
    this.tableName = tableName;
  }

  /**
   * Execute a query with parameters
   * @param {string} query - SQL query
   * @param {Array} params - Query parameters
   * @param {Object} options - Query options
   * @returns {Promise<Object>} - Query result
   */
  async executeQuery(query, params = [], options = {}) {
    const { useTransaction = false, client = null } = options;
    let localClient = null;
    let result = null;
    
    try {
      // Use provided client or get a new one from the pool
      localClient = client || (useTransaction ? await getPgClient() : null);
      
      // If using transaction, begin transaction
      if (useTransaction && !client) {
        await localClient.query('BEGIN');
      }
      
      // Execute query
      if (localClient) {
        result = await localClient.query(query, params);
      } else {
        result = await pgPool.query(query, params);
      }
      
      // If using transaction and no client was provided, commit transaction
      if (useTransaction && !client) {
        await localClient.query('COMMIT');
      }
      
      return result;
    } catch (error) {
      // If using transaction and no client was provided, rollback transaction
      if (useTransaction && localClient && !client) {
        try {
          await localClient.query('ROLLBACK');
        } catch (rollbackError) {
          logger.error('Error rolling back transaction', { 
            error: rollbackError.message, 
            stack: rollbackError.stack,
            query,
            params
          });
        }
      }
      
      logger.error('Error executing query', { 
        error: error.message, 
        stack: error.stack,
        query,
        params
      });
      
      throw error;
    } finally {
      // Release client if it was created locally
      if (localClient && !client) {
        localClient.release();
      }
    }
  }

  // CRUD operations...
}
```

We also implemented a similar repository for MongoDB:

```javascript
class MongoRepository {
  constructor(model) {
    this.model = model;
  }

  /**
   * Ensure MongoDB is connected
   * @returns {Promise<void>}
   */
  async ensureConnection() {
    if (mongoose.connection.readyState !== 1) {
      await connectMongo();
    }
  }

  // CRUD operations...
}
```

These repositories provided:
- CRUD operations for both PostgreSQL and MongoDB
- Transaction support
- Error handling and logging
- Query building

#### 3. Caching Service

We implemented a caching service using Redis:

```javascript
class CacheService {
  /**
   * Set a value in the cache
   * @param {string} key - Cache key
   * @param {any} value - Value to cache
   * @param {Object} options - Cache options
   * @returns {Promise<boolean>} - Success flag
   */
  async set(key, value, options = {}) {
    const { ttl = null } = options;
    
    try {
      const stringValue = typeof value === 'string' ? value : JSON.stringify(value);
      
      if (ttl) {
        await redisClient.set(key, stringValue, 'EX', ttl);
      } else {
        await redisClient.set(key, stringValue);
      }
      
      return true;
    } catch (error) {
      logger.error('Error setting cache value', { 
        error: error.message, 
        stack: error.stack,
        key
      });
      return false;
    }
  }

  /**
   * Get a value from the cache with a fallback function
   * @param {string} key - Cache key
   * @param {Function} fallback - Fallback function
   * @param {Object} options - Cache options
   * @returns {Promise<any>} - Cached or fallback value
   */
  async getOrSet(key, fallback, options = {}) {
    const { ttl = null, parse = true } = options;
    
    try {
      // Try to get from cache
      const cachedValue = await this.get(key, { parse });
      
      if (cachedValue !== null) {
        return cachedValue;
      }
      
      // If not in cache, execute fallback
      const value = await fallback();
      
      // Cache the result
      await this.set(key, value, { ttl });
      
      return value;
    } catch (error) {
      logger.error('Error in getOrSet cache operation', { 
        error: error.message, 
        stack: error.stack,
        key
      });
      
      // If cache operation fails, still try to execute fallback
      return await fallback();
    }
  }

  // Other cache operations...
}
```

This caching service provided:
- Simple key-value caching
- TTL (Time To Live) support
- Fallback function for cache misses
- Error handling and logging

#### 4. Migration Service

We implemented a migration service to manage database schema changes:

```javascript
class MigrationService {
  constructor() {
    this.migrationsDir = path.join(__dirname, '../../migrations');
    this.pgMigrations = [];
    this.mongoMigrations = [];
  }

  /**
   * Initialize the migration service
   * @returns {Promise<void>}
   */
  async init() {
    // Load migration files
    await this.loadMigrations();
    
    // Create migrations table if it doesn't exist
    await this.createMigrationsTable();
  }

  /**
   * Run all pending migrations
   * @param {Object} options - Migration options
   * @returns {Promise<void>}
   */
  async migrate(options = {}) {
    const { mongoUri = process.env.MONGODB_URI } = options;
    
    try {
      await this.init();
      
      // Apply PostgreSQL migrations
      const pendingPgMigrations = await this.getPendingPgMigrations();
      
      if (pendingPgMigrations.length === 0) {
        logger.info('No pending PostgreSQL migrations');
      } else {
        logger.info('Applying PostgreSQL migrations', { count: pendingPgMigrations.length });
        
        for (const migration of pendingPgMigrations) {
          await this.applyPgMigration(migration);
        }
        
        logger.info('PostgreSQL migrations completed successfully');
      }
      
      // Apply MongoDB migrations
      if (mongoUri) {
        await this.applyMongoMigrations(mongoUri);
      } else {
        logger.warn('MongoDB URI not provided, skipping MongoDB migrations');
      }
      
      logger.info('All migrations completed successfully');
    } catch (error) {
      logger.error('Migration failed', { 
        error: error.message, 
        stack: error.stack
      });
      throw error;
    }
  }

  // Other migration methods...
}
```

This migration service provided:
- Automatic discovery of migration files
- Tracking of applied migrations
- Transaction support for PostgreSQL migrations
- Support for both PostgreSQL and MongoDB migrations

#### 5. Docker Compose for Development

We created a Docker Compose file to set up the development environment:

```yaml
version: '3.8'

services:
  postgres:
    image: postgres:14-alpine
    container_name: dr_assistant_postgres
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: dr_assistant
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5

  mongodb:
    image: mongo:5.0
    container_name: dr_assistant_mongodb
    environment:
      MONGO_INITDB_ROOT_USERNAME: mongo
      MONGO_INITDB_ROOT_PASSWORD: mongo
      MONGO_INITDB_DATABASE: dr_assistant
    ports:
      - "27017:27017"
    volumes:
      - mongodb_data:/data/db
    healthcheck:
      test: echo 'db.runCommand("ping").ok' | mongo localhost:27017/dr_assistant --quiet
      interval: 10s
      timeout: 5s
      retries: 5

  redis:
    image: redis:6-alpine
    container_name: dr_assistant_redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

volumes:
  postgres_data:
  mongodb_data:
  redis_data:
```

This Docker Compose file provided:
- PostgreSQL, MongoDB, and Redis services
- Persistent volumes for data
- Health checks for all services
- Environment variables for configuration

## Challenges and Solutions

### Challenge 1: Managing Multiple Database Technologies

**Challenge:** We needed to manage connections to three different database technologies (PostgreSQL, MongoDB, and Redis) in a consistent way.

**Solution:** We implemented a unified database configuration module that provided a consistent interface for all database technologies:

1. **Consistent Configuration**: We used environment variables for all database configurations, with sensible defaults.

2. **Connection Management**: We implemented connection pooling for PostgreSQL, lazy connection for MongoDB, and a singleton client for Redis.

3. **Error Handling**: We implemented consistent error handling for all database connections, with detailed logging.

4. **Graceful Shutdown**: We provided a method to gracefully close all database connections.

```javascript
// Helper function to close all database connections
const closeConnections = async () => {
  try {
    await pgPool.end();
    logger.info('PostgreSQL pool has ended');
  } catch (error) {
    logger.error('Error closing PostgreSQL pool', { 
      error: error.message, 
      stack: error.stack
    });
  }
  
  try {
    if (mongoose.connection.readyState !== 0) {
      await mongoose.connection.close();
      logger.info('MongoDB connection closed');
    }
  } catch (error) {
    logger.error('Error closing MongoDB connection', { 
      error: error.message, 
      stack: error.stack
    });
  }
  
  try {
    await redisClient.quit();
    logger.info('Redis connection closed');
  } catch (error) {
    logger.error('Error closing Redis connection', { 
      error: error.message, 
      stack: error.stack
    });
  }
};
```

### Challenge 2: Implementing the Repository Pattern

**Challenge:** We needed to implement a repository pattern that would work for both PostgreSQL and MongoDB, providing a consistent interface for database operations.

**Solution:** We implemented separate base repository classes for PostgreSQL and MongoDB, with similar interfaces:

1. **BaseRepository for PostgreSQL**: Provided CRUD operations for PostgreSQL tables, with transaction support.

2. **MongoRepository for MongoDB**: Provided CRUD operations for MongoDB collections, with connection management.

3. **Common Interface**: Both repositories provided similar methods (findAll, findById, create, update, delete) with similar parameters.

4. **Error Handling**: Both repositories implemented consistent error handling and logging.

```javascript
// Example of similar interfaces
// PostgreSQL
const user = await userRepository.findById(userId);

// MongoDB
const clinicalNote = await clinicalNoteRepository.findById(noteId);
```

### Challenge 3: Implementing Caching Strategies

**Challenge:** We needed to implement caching strategies to optimize performance, especially for frequently accessed data.

**Solution:** We implemented a caching service using Redis, with support for various caching patterns:

1. **Simple Caching**: Set and get values with optional TTL.

2. **Cache-Aside Pattern**: Get a value from the cache, or execute a fallback function and cache the result.

3. **Cache Invalidation**: Delete cache keys when data changes.

4. **Pattern-Based Invalidation**: Delete cache keys matching a pattern.

```javascript
// Example of cache-aside pattern
const user = await cacheService.getOrSet(
  `user:${userId}`,
  async () => await userRepository.findById(userId),
  { ttl: 3600 } // 1 hour
);
```

## Impact and Outcomes

The implementation of MVP-007 had several significant impacts:

1. **Consistent Database Access**: All services now had a consistent way to access the database, reducing code duplication and improving maintainability.

2. **Improved Performance**: The caching strategies and connection pooling improved application performance, especially for frequently accessed data.

3. **Robust Error Handling**: The comprehensive error handling and logging made it easier to diagnose and fix database-related issues.

4. **Simplified Development**: The repository pattern simplified development by providing a clear interface for database operations.

5. **Easier Testing**: The repository pattern made it easier to mock database operations for testing.

## Lessons Learned

1. **Consistent Interfaces Matter**: Having consistent interfaces for different database technologies made it easier to switch between them and reduced cognitive load for developers.

2. **Error Handling is Crucial**: Comprehensive error handling and logging were essential for diagnosing and fixing database-related issues.

3. **Connection Management is Important**: Proper connection management (pooling, lazy connection, graceful shutdown) was crucial for application stability and performance.

4. **Caching Requires Strategy**: Effective caching required a clear strategy for what to cache, how long to cache it, and when to invalidate the cache.

5. **Repository Pattern Simplifies Development**: The repository pattern simplified development by providing a clear interface for database operations and hiding the complexity of database access.

## Connection to Other Tickets

MVP-007 was directly connected to several other tickets:

- **MVP-004 (Define Data Model & Database Schema)**: MVP-007 implemented the database infrastructure based on the data model defined in MVP-004.

- **MVP-006 (Implement Authentication & Authorization)**: MVP-007 provided the database infrastructure used by the authentication service.

- **MVP-008 (Implement User Management)**: The user management service would use the database infrastructure implemented in MVP-007.

- **MVP-009 (Implement Patient Management)**: The patient management service would use the database infrastructure implemented in MVP-007.

## Conclusion

MVP-007 established the database infrastructure for the Dr. Assistant application. By implementing a robust database layer with connection management, repositories, and caching, we created a solid foundation for all services to interact with the database in a consistent and efficient way.

The polyglot persistence approach, with PostgreSQL for structured data, MongoDB for flexible schema data, and Redis for caching, gave us the best of all worlds: the reliability and consistency of a relational database, the flexibility of a document database, and the performance of an in-memory cache.

As we move forward with implementing specific features, the database infrastructure implemented in MVP-007 will continue to provide a solid foundation for all database operations, ensuring consistency, performance, and maintainability throughout the application lifecycle.
